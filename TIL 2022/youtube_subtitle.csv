x,y
1,so when you're learning you know the
2,0:02
3,basics of algorithmic
4,0:04
5,models now you'll have to confront
6,0:06
7,questions of algorithmic bias
8,0:08
9,and fairness it won't be if you're
10,0:10
11,interested in that go
12,0:11
13,take the class in the philosophy
14,0:12
15,department it will be built into the
16,0:14
17,core classes in cs
18,0:33
19,[Music]
20,0:39
21,i think we're already witnessing some of
22,0:41
23,the uh the harm
24,0:42
25,and damage being made by the adoption of
26,0:45
27,algorithms and ai
28,0:47
29,in like the biases that exist
30,0:50
31,in some of the tools and the spread of
32,0:52
33,discrimination and
34,0:54
35,distrust in the media and all of that to
36,0:57
37,deal with that
38,0:58
39,potential threat and harm like who do
40,1:00
41,you think
42,1:01
43,is most important to be educated and
44,1:03
45,like will
46,1:04
47,informed consumers be most critical
48,1:07
49,or informed developers or inform the
50,1:10
51,government
52,1:13
53,[Music]
54,1:17
55,well i think all of the above is
56,1:19
57,important and
58,1:20
59,at the end of the day i wouldn't leave
60,1:23
61,this just to consumers however
62,1:25
63,because that's a kind of one by one
64,1:28
65,approach
66,1:29
67,that doesn't leave possible a kind of
68,1:31
69,broader or collective strategy
70,1:34
71,so you know some people say they're so
72,1:36
73,fed up with facebook
74,1:38
75,that the only responsible thing to do is
76,1:40
77,to delete facebook from your phone
78,1:42
79,or your smartphone is so filled with
80,1:45
81,notification devices and various ways to
82,1:47
83,hack your attention
84,1:49
85,that you should you know like turn the
86,1:50
87,color off and make it grayscale
88,1:52
89,and like that's the power of a consumer
90,1:54
91,if that's the sum
92,1:56
93,total of our kind of possible actions
94,1:59
95,that i think human beings are going to
96,2:01
97,lose out in the end
98,2:02
99,i'll give you just one example in the
100,2:04
101,space of say you know content moderation
102,2:06
103,or content regulation
104,2:07
105,what i have in mind so in the united
106,2:10
107,states
108,2:10
109,when you used to be able to go to a
110,2:12
111,movie theater
112,2:13
113,the movies are all rated you know g or
114,2:16
115,pg or r
116,2:18
117,or no content that is not a law that was
118,2:21
119,passed by
120,2:22
121,the us government it's a voluntary
122,2:24
123,industry collaboration
124,2:27
125,to self-regulate and to give information
126,2:29
127,to consumers so they can make informed
128,2:31
129,decisions about whether or not they wish
130,2:33
131,to
132,2:34
133,allow their children to see a movie or
134,2:36
135,if it's rated r there's an age cut off
136,2:38
137,that's an example that sits above the
138,2:40
139,consumer and is below the government
140,2:42
141,so that's i want to emphasize our
142,2:44
143,choices are not just
144,2:45
145,let the individual user decide or let
146,2:47
147,government regulate
148,2:48
149,we have lots of other options in between
150,2:55
151,[Music]
152,2:56
153,one of the great things about being a
154,2:58
155,university is that we get to teach
156,3:01
157,19 20 21 year old kids and
158,3:05
159,these are the kids that are going to end
160,3:06
161,up in 10 years in positions of
162,3:08
163,leadership and responsibility
164,3:10
165,so we can think actually about how do we
166,3:13
167,want to educate
168,3:14
169,people for the next 10 years so that 30
170,3:16
171,years from now
172,3:17
173,the world looks like a different place i
174,3:19
175,mean i think it's a great transition to
176,3:21
177,your ethics curriculum that you're
178,3:23
179,supporting campus
180,3:24
181,hey can you tell us about the new
182,3:27
183,curriculum how you're adopting it and
184,3:29
185,is there any early wins that you have
186,3:31
187,witnessed
188,3:32
189,well we're doing a bunch of really
190,3:34
191,exciting things much of it
192,3:36
193,driven by hai style thinking and with
194,3:39
195,support from people like you so i'm
196,3:42
197,really excited about
198,3:43
199,two different efforts that complement
200,3:46
201,each other
202,3:47
203,one is what we call embedded ethics and
204,3:49
205,the idea
206,3:50
207,is not to tell the computer science
208,3:52
209,professor who doesn't have
210,3:54
211,the kind of philosophical training to
212,3:56
213,include
214,3:57
215,genuinely robust content in ethics in
216,4:00
217,his or her courses
218,4:01
219,but now working in collaboration with
220,4:04
221,someone who does have that training
222,4:05
223,we're developing modules for each of the
224,4:08
225,core courses
226,4:09
227,that will expose every single student at
228,4:12
229,stanford who's a cs major to
230,4:14
231,ethical frameworks and ethical questions
232,4:16
233,about ai
234,4:17
235,and you know computer science more more
236,4:19
237,broadly so when you're learning
238,4:21
239,you know the basics of algorithmic
240,4:23
241,models now
242,4:25
243,you'll have to confront questions of
244,4:27
245,algorithmic bias
246,4:28
247,and fairness it will be built into the
248,4:30
249,core classes to cs
250,4:32
251,now there's a second initiative and this
252,4:34
253,is one that i'm also personally involved
254,4:35
255,in which is
256,4:37
257,a large introductory course and the
258,4:39
259,large introductory course combines me
260,4:42
261,the
262,4:42
263,philosopher a colleague who's a social
264,4:44
265,scientist that has experience in public
266,4:46
267,policy
268,4:47
269,and then a computer science professor so
270,4:49
271,this is the only course we know
272,4:51
273,at stanford at least that combines
274,4:53
275,technical assignments
276,4:55
277,policy memos and philosophy papers in
278,4:57
279,the same class
280,4:59
281,so we wanted to make this a large class
282,5:01
283,it enrolls about 300 people
284,5:03
285,right that's great i mean i think that's
286,5:06
287,wonderful that
288,5:07
289,stanford has adopted that curriculum and
290,5:09
291,offered to
292,5:10
293,the young students so those are
294,5:13
295,available
296,5:13
297,those learnings and curriculums are
298,5:15
299,available for yes
300,5:17
301,all the materials are our creative
302,5:19
303,commons license and
304,5:20
305,they'll be assembled on a website
306,5:21
307,together the website for the class i
308,5:23
309,just
310,5:24
311,described is cs182.stanford.edu you can
312,5:27
313,download all the case studies all of the
314,5:29
315,readings as long as they're not
316,5:31
317,copyrighted
318,5:32
319,and you know similar for the modules
320,5:33
321,that we're developing in the embedded
322,5:35
323,ethics initiative we would love for
324,5:37
325,other people to use them in their
326,5:39
327,company at their other universities
328,5:41
329,they're meant for for everybody
330,5:46
331,[Music]
332,5:49
333,according to that we don't necessarily
334,5:52
335,exercise
336,5:52
337,everything we learned in kindergarten
338,5:54
339,are you hopeful that
340,5:56
341,those who took those courses and lessons
342,5:58
343,will
344,5:59
345,practice what they learned throughout
346,6:01
347,their career
348,6:02
349,yeah that's a great question i have to
350,6:04
351,remind people that
352,6:05
353,there are at least three different
354,6:07
355,levels of thinking about ethics
356,6:09
357,and the first level and the least
358,6:11
359,interesting level to me
360,6:13
361,at least is personal ethics human beings
362,6:16
363,are imperfect
364,6:16
365,almost no one is a moral saint we all
366,6:19
367,suffer from various flaws
368,6:21
369,and so the goal for us shouldn't be that
370,6:24
371,we need an ethics course as if it's a
372,6:26
373,kind of
374,6:26
375,like a vaccine against future bad
376,6:29
377,behavior
378,6:30
379,much more interesting are a second level
380,6:32
381,and a third level of ethics the second
382,6:34
383,level is
384,6:34
385,professional ethics what are the
386,6:36
387,professional norms and structures
388,6:38
389,that should bind people together in
390,6:40
391,their professional activity
392,6:41
393,now the classic one is you know doctors
394,6:43
395,or medical healthcare providers who have
396,6:46
397,the hippocratic oath of do no harm
398,6:48
399,plus a whole set of other norms that
400,6:50
401,guide their own practice
402,6:52
403,um i think we could develop a much
404,6:55
405,broader set of professional norms for ai
406,6:57
407,scientists
408,6:59
409,and that would be a good contribution
410,7:01
411,and then of course the
412,7:02
413,the final level the third level is
414,7:04
415,political or social ethics
416,7:06
417,how should we think about the
418,7:07
419,institutions that shape our own behavior
420,7:10
421,and how do we design better institutions
422,7:12
423,so that
424,7:13
425,whatever human behavior is it's
426,7:15
427,channeled in general to a better
428,7:16
429,direction
430,7:17
431,yeah we all knew that you shouldn't lie
432,7:19
433,cheat or steal
434,7:20
435,don't be lance armstrong the olympic
436,7:23
437,bicyclist
438,7:23
439,or the the tour de france winner who
440,7:25
441,doped himself in order to win
442,7:27
443,don't be elizabeth holmes the stanford
444,7:29
445,dropout who created theranos but was
446,7:32
447,deceiving people about the power of the
448,7:33
449,technology
450,7:34
451,we don't need ender's classes to tell
452,7:36
453,people don't lie cheat or steal
454,7:38
455,if you didn't learn that by the time you
456,7:40
457,showed up to stanford it's already too
458,7:41
459,late
460,7:42
461,what we need are ethics classes at a
462,7:44
463,level of institutional arrangements
464,7:46
465,confronting value trade-offs intentions
466,7:48
467,the kinds of things which are
468,7:50
469,part and parcel of any responsible
470,7:52
471,person's life
472,7:53
473,fantastic thank you so much again for
474,7:55
475,your time and
476,7:56
477,sharing your thoughts all right so hope
478,7:58
479,we can do this in person
480,8:00
481,at some point in the future wouldn't
482,8:01
483,that be nice i'd love to meet people
484,8:04
485,uh you know in your part of the world
486,8:06
487,and have conversations as well
488,8:07
489,and would love to welcome anyone here to
490,8:10
491,campus
492,8:10
493,in the future too thank you thank you so
494,8:13
495,much
496,8:14
497,great super thank you so much
