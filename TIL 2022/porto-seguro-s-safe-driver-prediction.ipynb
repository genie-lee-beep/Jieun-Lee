{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-15T15:44:03.869290Z","iopub.execute_input":"2022-03-15T15:44:03.870028Z","iopub.status.idle":"2022-03-15T15:44:03.925092Z","shell.execute_reply.started":"2022-03-15T15:44:03.869889Z","shell.execute_reply":"2022-03-15T15:44:03.924117Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"모델 구조\n\nMeta Data 생성  \n불균형 데이터 조정  \n결측치 처리  \nFeature Engineering  \n범주형 변수 인코딩  \nMean Encoding  \nOne-hot Encoding  \nPolynomialFeatures  \nFeature Selection  \nVarianceThreshold  \nSelectFromModel  \nFeature Scaling  \nModeling  \n\n<대회 설명>  \n\nPorto Seguro는 브라질의 자동차 보험 회사.    \n보험 청구할 확률이 높은 차주한테 보험료를 높게 청구하고, 아닌 차주는 낮게 보험료를 청구     \n\n<대회 목적>\n\n어떤 차주가 내년에 보험 청구 할 확률 예측\n\n<데이터 특징>\n\n테스트 데이터 > 훈련 데이터\n결측치의 값이 -1로 주어짐\n실제 기업 데이터이기 때문에 feature를 비식별화 해놨음 (그렇기 때문에 난이도가 높은 쪽에 속함),  \n지니 예상으로는 사고 횟수, 보험같이든 가족 수 등등이 아닐까?         \ntarget = 0 : 보험 청구 no / target = 1 : 보험 청구 yes  \n<데이터 평가>\n\npredict_proba라는 함수를 사용해서 값을 예측 (확률 값)  \n\n대회의 평가지표는 특이하게 Normalized Gini Coefficient를 사용함  \n지니 계수 : 경제적 불평등을 계수화할 때 주로 사용하는 지표\nNormalized Gini Coefficient를 사용하는 이유:  \nImbalanced Class를 평가를 위한 임계값을 어떻게 정하느냐에 따라 예측값이 바뀜  \n= ROC 커브로 확인 (면적으로 스코어를 매김) >> 비슷한 목적으로 Gini Coefficient가 쓰임  \n= gini = 2 * AUC - 1\n\n0~0.5의 값을 가짐\n0.5에 가까울수록 좋은 분석임.\n\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# from sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.utils import shuffle\nfrom sklearn.ensemble import RandomForestClassifier\n\npd.set_option('display.max_columns', 100)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:44:03.927336Z","iopub.execute_input":"2022-03-15T15:44:03.927895Z","iopub.status.idle":"2022-03-15T15:44:05.649542Z","shell.execute_reply.started":"2022-03-15T15:44:03.927844Z","shell.execute_reply":"2022-03-15T15:44:05.648511Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/porto-seguros-safe-driver-prediction-train-data/train.csv')\ntest = pd.read_csv('../input/porto-seguros-safe-driver-prediction-test-data/test.csv')\n# sub = pd.read_csv(\"/kaggle/input/bike-sharing-demand/sampleSubmission.csv\")                   \n","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:44:05.651039Z","iopub.execute_input":"2022-03-15T15:44:05.651361Z","iopub.status.idle":"2022-03-15T15:44:18.373676Z","shell.execute_reply.started":"2022-03-15T15:44:05.651292Z","shell.execute_reply":"2022-03-15T15:44:18.372662Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:44:18.376119Z","iopub.execute_input":"2022-03-15T15:44:18.376494Z","iopub.status.idle":"2022-03-15T15:44:18.426113Z","shell.execute_reply.started":"2022-03-15T15:44:18.376438Z","shell.execute_reply":"2022-03-15T15:44:18.425077Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train.tail()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:44:18.427284Z","iopub.execute_input":"2022-03-15T15:44:18.427592Z","iopub.status.idle":"2022-03-15T15:44:18.471724Z","shell.execute_reply.started":"2022-03-15T15:44:18.427558Z","shell.execute_reply":"2022-03-15T15:44:18.470750Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:44:18.473667Z","iopub.execute_input":"2022-03-15T15:44:18.474000Z","iopub.status.idle":"2022-03-15T15:44:18.480664Z","shell.execute_reply.started":"2022-03-15T15:44:18.473956Z","shell.execute_reply":"2022-03-15T15:44:18.479837Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train.drop_duplicates()\ntrain.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:44:18.481702Z","iopub.execute_input":"2022-03-15T15:44:18.482396Z","iopub.status.idle":"2022-03-15T15:44:19.566507Z","shell.execute_reply.started":"2022-03-15T15:44:18.482345Z","shell.execute_reply":"2022-03-15T15:44:19.565533Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"No duplicate rows, so that's fine.","metadata":{}},{"cell_type":"code","source":"test.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:44:19.568723Z","iopub.execute_input":"2022-03-15T15:44:19.569016Z","iopub.status.idle":"2022-03-15T15:44:19.576216Z","shell.execute_reply.started":"2022-03-15T15:44:19.568983Z","shell.execute_reply":"2022-03-15T15:44:19.574873Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"test.drop_duplicates()\ntest.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:44:19.577866Z","iopub.execute_input":"2022-03-15T15:44:19.578195Z","iopub.status.idle":"2022-03-15T15:44:21.173414Z","shell.execute_reply.started":"2022-03-15T15:44:19.578161Z","shell.execute_reply":"2022-03-15T15:44:21.172229Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:44:21.176962Z","iopub.execute_input":"2022-03-15T15:44:21.178095Z","iopub.status.idle":"2022-03-15T15:44:21.264079Z","shell.execute_reply.started":"2022-03-15T15:44:21.178045Z","shell.execute_reply":"2022-03-15T15:44:21.263414Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Meta Data\nrole: input, ID, target  \nlevel: nominal, interval, ordinal, binary  \nkeep: True or False  \ndtype: int, float, str  ","metadata":{}},{"cell_type":"code","source":"data = []\nfor f in train.columns:\n    if f =='target':\n        role = 'target'\n    elif f == 'id':\n        role = 'id'\n    else:\n        role = 'input'\n   \n    # if there is the bin string in the f or 'target' column, level is binary    \n    if 'bin' in f or f == 'target':\n        level = 'binary'\n    elif 'cat' in f or f == 'id':\n        level = 'nominal'\n    # if trian's columns' dtypes are float, level is float.  \n    elif train[f].dtype == float:\n        level = 'float'\n    elif train[f].dtype == int:\n        level = 'ordinal'\n        \n   # Initialize keep to True for all variables except for id\n    keep = True\n    if f == 'id':\n        keep = False\n    \n\n    dtype = train[f].dtype\n    \n    # DataFrame으로 만들기 위해 리스트에 append하기 전에 딕셔너리 타입으로 만들어주었음\n    f_dict={\n        'varname':f,\n        'roel':role,\n        'level':level,\n        'keep':keep,\n        'dtype': dtype\n    }\n    \n    data.append(f_dict)\n    \n# 변수의 이름을 인덱스로 하는 데이터프레임을 만들어줌    \nmeta = pd.DataFrame(data, columns = ['varname', 'role', 'level', ' keep', ' dtype'])\nmeta.set_index('varname', inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:44:21.265505Z","iopub.execute_input":"2022-03-15T15:44:21.265820Z","iopub.status.idle":"2022-03-15T15:44:21.281086Z","shell.execute_reply.started":"2022-03-15T15:44:21.265773Z","shell.execute_reply":"2022-03-15T15:44:21.280153Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"data = []\nfor f in train.columns:\n    # Defining the role\n    if f == 'target':\n        role = 'target'\n    elif f == 'id':\n        role = 'id'\n    else:\n        role = 'input'\n         \n    # Defining the level\n    if 'bin' in f or f == 'target':\n        level = 'binary'\n    elif 'cat' in f or f == 'id':\n        level = 'nominal'\n    elif train[f].dtype == float:\n        level = 'interval'\n    elif train[f].dtype == int:\n        level = 'ordinal'\n        \n    # Initialize keep to True for all variables except for id\n    keep = True\n    if f == 'id':\n        keep = False\n    \n    # trian set의 컬럼별 원래 dtype을 적어라. \n    dtype = train[f].dtype\n    \n    # Creating a Dict that contains all the metadata for the variable\n    f_dict = {\n        'varname': f,   # f에 들어가 있는 모든 train의 colums들 \n        'role': role,\n        'level': level,\n        'keep': keep,\n        'dtype': dtype\n    }\n    data.append(f_dict)\n    \nmeta = pd.DataFrame(data, columns=['varname', 'role', 'level', 'keep', 'dtype'])\nmeta.set_index('varname', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:44:21.283785Z","iopub.execute_input":"2022-03-15T15:44:21.284086Z","iopub.status.idle":"2022-03-15T15:44:21.303318Z","shell.execute_reply.started":"2022-03-15T15:44:21.284050Z","shell.execute_reply":"2022-03-15T15:44:21.302193Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"meta","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:44:21.304760Z","iopub.execute_input":"2022-03-15T15:44:21.304996Z","iopub.status.idle":"2022-03-15T15:44:21.337283Z","shell.execute_reply.started":"2022-03-15T15:44:21.304968Z","shell.execute_reply":"2022-03-15T15:44:21.336555Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Example to extract all nominal variables that are not dropped","metadata":{}},{"cell_type":"code","source":"meta[(meta.level == 'nominal') & (meta.keep)].index","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:44:21.338671Z","iopub.execute_input":"2022-03-15T15:44:21.339409Z","iopub.status.idle":"2022-03-15T15:44:21.350140Z","shell.execute_reply.started":"2022-03-15T15:44:21.339341Z","shell.execute_reply":"2022-03-15T15:44:21.349166Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Q size 함수를 해주는 이유?\npd.DataFrame({'count' : meta.groupby(['role', 'level'])['role'].size()}).reset_index()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:44:21.351386Z","iopub.execute_input":"2022-03-15T15:44:21.351627Z","iopub.status.idle":"2022-03-15T15:44:21.371866Z","shell.execute_reply.started":"2022-03-15T15:44:21.351597Z","shell.execute_reply":"2022-03-15T15:44:21.371217Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"위에서 만들어준 정리를 통해 유형별로 데이터를 어떻게 다룰 것인가를 쉽게 선택할 수 있음\nex. 연속형 변수의 경우 통계적 방법, 범주형 변수의 경우 시각화를 통한 탐색 등","metadata":{}},{"cell_type":"code","source":"# Q .index를 해주는 이유?\n# 우리는 지금 varname을 index로 만들어 준 상태. level이 interval이면서, keep인 index들을 Interval에 담겠다.\nInterval = meta[(meta[\"level\"] == \"interval\") & (meta[\"keep\"])].index","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:44:21.373641Z","iopub.execute_input":"2022-03-15T15:44:21.374612Z","iopub.status.idle":"2022-03-15T15:44:21.380728Z","shell.execute_reply.started":"2022-03-15T15:44:21.374541Z","shell.execute_reply":"2022-03-15T15:44:21.379554Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"Interval","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:44:21.382082Z","iopub.execute_input":"2022-03-15T15:44:21.382546Z","iopub.status.idle":"2022-03-15T15:44:21.394805Z","shell.execute_reply.started":"2022-03-15T15:44:21.382511Z","shell.execute_reply":"2022-03-15T15:44:21.393986Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# describe를 통해 interval 변수들의 통계량을 확인 \ntrain[Interval].describe()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:44:21.396044Z","iopub.execute_input":"2022-03-15T15:44:21.396758Z","iopub.status.idle":"2022-03-15T15:44:21.705698Z","shell.execute_reply.started":"2022-03-15T15:44:21.396718Z","shell.execute_reply":"2022-03-15T15:44:21.704585Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"**reg variables**\n- only ps_reg_03 has missing values  \n- the range (min to max) differs between the variables. We could apply scaling (e.g. StandardScaler), but it depends on the classifier we will want to use.\n\n**car variables**\n- ps_car_12 and ps_car_15 have missing values\n- again, the range differs and we could apply scaling.\n\n**calc variables**\n- no missing values\n- this seems to be some kind of ratio as the maximum is 0.9\n- all three _calc variables have very similar distributions\n\n**Overall**, we can see that the range of the interval variables is rather small. Perhaps some transformation (e.g. log) is already applied in order to anonymize the data?","metadata":{}},{"cell_type":"markdown","source":"### Ordinal variables","metadata":{}},{"cell_type":"code","source":"# .index : 데이터 프레임이 아닌 인덱스만 뽑는 것.\nOrdinal = meta[(meta[\"level\"] == \"ordinal\") & (meta[\"keep\"])].index\nOrdinal\n# train[Ordinal].describe()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:48:28.543499Z","iopub.execute_input":"2022-03-15T15:48:28.543820Z","iopub.status.idle":"2022-03-15T15:48:28.552203Z","shell.execute_reply.started":"2022-03-15T15:48:28.543785Z","shell.execute_reply":"2022-03-15T15:48:28.551534Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"Ordinal = meta[(meta[\"level\"] == \"ordinal\") & (meta[\"keep\"])].index\ntrain[Ordinal].describe()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:48:53.099670Z","iopub.execute_input":"2022-03-15T15:48:53.100522Z","iopub.status.idle":"2022-03-15T15:48:53.512795Z","shell.execute_reply.started":"2022-03-15T15:48:53.100410Z","shell.execute_reply":"2022-03-15T15:48:53.511779Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"### Binary variables","metadata":{}},{"cell_type":"code","source":"Binary = meta[(meta[\"level\"] == 'binary') & (meta[\"keep\"])].index\n# describe를 통해 Ordinal 변수들의 통계량을 확인 \n\ntrain[Binary].describe()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:44:22.088477Z","iopub.execute_input":"2022-03-15T15:44:22.088741Z","iopub.status.idle":"2022-03-15T15:44:22.454018Z","shell.execute_reply.started":"2022-03-15T15:44:22.088697Z","shell.execute_reply":"2022-03-15T15:44:22.453037Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"위의 통계량을 바탕으로 Binary 데이터를 살펴보자  \n1) 결측치 확인  \n결측치 X  \n\n2) 변수들 사이의 범위 확인  \nBinary 데이터이기 때문에 범위를 확인할 필요는 없다. (0 or 1)    \n\n3) Target 변수 확인    \nBinary에는 Target 변수까지 포함되어 있다.  \n그렇기 때문에 Target 변수에 대한 통계량을 확실하게 짚고 가야한다  \nTarget 데이터의 평균을 살펴보면 이 대회의 핵심을 알 수 있다.  \n데이터는 0 or 1 이기 때문에 균형이 맞기 위해선 평균이 0.5가 되어야 한다.  \n하지만 Target 데이터의 평균은 0.0364로 보인다. (굉장히 Imbalanced하다. 0이 훨씬 많아보인다)  \n대회의 Metric으로 Normalized Gini Coefficient를 사용하는 이유  \n\n\n**Imbalanced Class 처리**  \n보험이라는 도메인 특성상 불균형적인 타겟값은 사실 일반적이다.  \n보험을 청구하는 경우 (1) 보다 하지 않는 경우 (0)이 굉장히 많은 것을 위에서 확인하였다.  \nImbalanced한 데이터는 일반적으로 Undersampling 혹은 Oversampling으로 처리한다.  \nUnderSampling: 0이 1보다 훨씬 많으므로 0인 데이터를 줄여 균형을 맞춰준다.  \nOverSampling: 0이 1보다 훨씬 많으므로 1인 데이터를 늘려 균형을 맞춰준다.  \n선택방법: 본인의 결정이지만 보통 데이터셋의 크기를 기준으로 선택한다.  \n데이터가 너무 많으면, 오버샘플링 시 너무 많은 Cost가 들어가게 된다(시간, 컴퓨팅파워)  \n이 커널은 데이터가 많은편이라고 판단하여 언더샘플링을 수행하였다.  \n\n커널을 떠나서 불균형 방법을 해소하는 방법은 완벽가이드 책에 나왔던 SMOTE란 방법이 있다. (오버샘플링에 해당)  \n참고: https://datascienceschool.net/view-notebook/c1a8dad913f74811ae8eef5d3bedc0c3/  \n추가로 Class_weights라는 방법도 존재한다. (불균형 class의 가중치를 조정)  \n참고: https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html  \n성능을 확인하고 어떤 방법으로 불균형을 해소할지 판단하면 된다.  ","metadata":{}},{"cell_type":"code","source":"# 언더샘플링 비율을 지정해주기 위함 \ndesired_apriori=0.10\n\n# target 변수의 클래스에 따른 인덱스 지정 \nidx_0 = train[train[\"target\"] == 0].index\nidx_1 = train[train[\"target\"] == 1].index\n\n# 지정해준 인덱스로 클래스의 길이(레코드 수) 지정\nnb_0 = len(train.loc[idx_0])\nnb_1 = len(train.loc[idx_1])\n\n# 언더샘플링 수행\nundersampling_rate = ((1-desired_apriori)*nb_1)/(nb_0*desired_apriori)\nundersampled_nb_0 = int(undersampling_rate*nb_0)\nprint('target=0에 대한 언더샘플링 비율: {}'.format(undersampling_rate))\nprint('언더샘플링 전 target=0 레코드의 개수: {}'.format(nb_0))\nprint('언더샘플링 후 target=0 레코드의 개수: {}'.format(undersampled_nb_0))\n\n# 언더샘플링 비율이 적용된 개수 만큼 랜덤하게 샘플을 뽑아서 그 인덱스를 저장\nundersampled_idx = shuffle(idx_0, random_state=37, n_samples=undersampled_nb_0)\n\n# 언더샘플링 인덱스와 클래스 1의 인덱스를 리스트로 저장\nidx_list = list(undersampled_idx) + list(idx_1)\n\n# Return undersample data frame\ndf_train = train.loc[idx_list].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:44:37.830527Z","iopub.execute_input":"2022-03-15T15:44:37.830849Z","iopub.status.idle":"2022-03-15T15:44:38.516169Z","shell.execute_reply.started":"2022-03-15T15:44:37.830818Z","shell.execute_reply":"2022-03-15T15:44:38.515208Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"### Data Quality Checks","metadata":{}},{"cell_type":"markdown","source":"Checking missing values  \nMissings are reprensented as -1","metadata":{}},{"cell_type":"code","source":"vars_with_missing = []\n\nfor f in train.columns:\n    missings = train[train[f] == -1][f].count()\n    if missings > 0:\n        vars_with_missing.append(f)\n        missings_perc = missings/train.shape[0]\n        \n        print('Variable {} has {} records ({:2%}) with missing values'.format(f,missings,missings_perc))\n        \nprint('In total, there are {} variables with missing values'.format(len(vars_with_missing)))\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2022-03-15T16:24:12.151830Z","iopub.execute_input":"2022-03-15T16:24:12.152127Z","iopub.status.idle":"2022-03-15T16:24:12.571040Z","shell.execute_reply.started":"2022-03-15T16:24:12.152096Z","shell.execute_reply":"2022-03-15T16:24:12.569879Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"- ps_car_03_cat and ps_car_05_cat have a large proportion of records with missing values. Remove these variables.\n- For the other categorical variables with missing values, we can leave the missing value -1 as such.\n- ps_reg_03 (continuous) has missing values for 18% of all records. Replace by the mean.\n- ps_car_11 (ordinal) has only 5 records with misisng values. Replace by the mode.\n- ps_car_12 (continuous) has only 1 records with missing value. Replace by the mean.\n- ps_car_14 (continuous) has missing values for 7% of all records. Replace by the mean.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import Imputer\n\n# Dropping the variables with too many missing values\nvars_to_drop = ['ps_car_03','ps_car_05']\ntrain.drop(vars_to_drop, inplace = Truem axis=1)  #axis=0이면 열을 중심으로 axis=1이면 행을 중심으로.\nmeta,loc[(var_to_drop),'keep'] = False    #지운 데이터를 메타 데이터에 반영한다.\n\n# Imputing with the mean or mode\n# Imputing: 결측치가 많은 결측자료가 있을 때 표준이나, 대표성이 있는 다른 데이터를 활용하여 대체 될 수 있는 값들로 계산하여 입력 하는 과정을 일컫는다.\n# 최빈값(mode): 가장 빈번하게 관찰/측정되는 값\n\nmean_imp = Imputer(missing_values = -1, strategy = 'mean', axis=0)\nmode_imp = Imputer(missing_values = -1, strategy = 'most_frequent', axis =0)\n#fit_transform 함수는 이차원 배열을 입력으로 받으므로 [[]]를 사용. 그러나 다시 대입할 때는 1차원으로 만들어줘야 되니까 ravel() 함수로 flatten.\ntrain['ps_reg_03'] = mean_imp.fit_transform(train[['ps_reg_03']]).ravel()\ntrain['ps_car_12'] = mean_imp.fit_transform(train[['ps_car_12']]).ravel()\ntrain['ps_car_14'] = mean_imp.fit_transform(train[['ps_car_14']]).ravel()\ntrain['ps_car_11'] = mode_imp.fit_transform(train[['ps_car_11']]).ravel()\n","metadata":{"execution":{"iopub.status.busy":"2022-03-15T17:06:31.464614Z","iopub.execute_input":"2022-03-15T17:06:31.465286Z","iopub.status.idle":"2022-03-15T17:06:31.473325Z","shell.execute_reply.started":"2022-03-15T17:06:31.465240Z","shell.execute_reply":"2022-03-15T17:06:31.472350Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"https://www.kaggle.com/kongnyooong/porto-seguro-eda-for-korean","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}