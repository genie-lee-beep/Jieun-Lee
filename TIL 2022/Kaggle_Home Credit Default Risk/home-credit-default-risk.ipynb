{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-20T12:28:12.394740Z","iopub.execute_input":"2022-03-20T12:28:12.395143Z","iopub.status.idle":"2022-03-20T12:28:12.437381Z","shell.execute_reply.started":"2022-03-20T12:28:12.395050Z","shell.execute_reply":"2022-03-20T12:28:12.436596Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"https://suhyun-cho.github.io/kaggle/kaggle-HomeCredit-default-risk-eda-and-FeatureEngineering_beginner/","metadata":{}},{"cell_type":"code","source":"# numpy and pandas for data manipulation\nimport numpy as np\nimport pandas as pd \n\n# sklearn preprocessing for dealing with categorical variables\nfrom sklearn.preprocessing import LabelEncoder\n\n# File system manangement\nimport os\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2022-03-20T12:28:12.439216Z","iopub.execute_input":"2022-03-20T12:28:12.439660Z","iopub.status.idle":"2022-03-20T12:28:13.658186Z","shell.execute_reply.started":"2022-03-20T12:28:12.439621Z","shell.execute_reply":"2022-03-20T12:28:13.657307Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Training data\napp_train = pd.read_csv('../input/home-credit/application_train.csv')\nprint('Training data shape: ', app_train.shape)\napp_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-20T12:28:13.659463Z","iopub.execute_input":"2022-03-20T12:28:13.659737Z","iopub.status.idle":"2022-03-20T12:28:19.510485Z","shell.execute_reply.started":"2022-03-20T12:28:13.659700Z","shell.execute_reply":"2022-03-20T12:28:19.509650Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Testing data features\napp_test = pd.read_csv('../input/application_test.csv')\nprint('Testing data shape: ', app_test.shape)\napp_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-20T12:28:19.511448Z","iopub.execute_input":"2022-03-20T12:28:19.511633Z","iopub.status.idle":"2022-03-20T12:28:19.618318Z","shell.execute_reply.started":"2022-03-20T12:28:19.511609Z","shell.execute_reply":"2022-03-20T12:28:19.617038Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# 이 노트북에서는 범주형변수의 유니크한 값이 2개일경우 Label encoding을 사용하고\n#그 이상일 경우 One-hot encoding을 사용한다.\nle=LabelEncoder()\nle_count=0\n\n# 컬럼별로 iterate 돌기\nfor col in app_train:\n    # df[리스트] \n    # dataframe의 경우 \n    if app_train[col].dtype=='object':\n        # 데이터타입이 object이고 값의 종류가 두개 이하일경우,\n        if len(list(app_train[col].unique())) <=2:  \n            \n            # train과 test에 동일하게 라벨인코딩을 하기위해 train기준으로 fit한값을 train,test에 동일하게 transform해줌\n            le.fit(app_train[col])\n            \n            # train-set, test-set 둘다 Transform\n            app_train[col]=le.transform(app_train[col])\n            app_test[col]=le.transform(app_test[col])\n            \n            # 라벨인코딩을 한 컬럼이 몇개인지 카운트\n            le_count+=1                                                    # Q 어떻게 카운트하는 건지?\nprint('%d columns were label encoded.' % le_count)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T12:28:19.619045Z","iopub.status.idle":"2022-03-20T12:28:19.619324Z","shell.execute_reply.started":"2022-03-20T12:28:19.619179Z","shell.execute_reply":"2022-03-20T12:28:19.619195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 위에서 Label-encoding적용 안한 나머지 범주형 변수에 One-hot encoding 적용\n# 위에서 이미 OBJECT 값이 2개인경우는 Label encoding 했으니까, 여기서는 바로 나머지 obect애 대해서는 get.cummies를 쫙 해준다.  \napp_train=pd.get_dummies(app_train)\napp_test=pd.get_dummies(app_test)\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-20T12:28:19.620683Z","iopub.status.idle":"2022-03-20T12:28:19.620979Z","shell.execute_reply.started":"2022-03-20T12:28:19.620838Z","shell.execute_reply":"2022-03-20T12:28:19.620854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TARGET변수는 train데이터에만 있지만 필요한 변수이기때문에 따로 빼두고나서 다시추가할것\ntrain_labels=app_train['TARGET']\n\n\"\"\"\n컬럼 수가 비교적 작은 train_set을 기준으로 컬럼수를 align해서 맞춰 준다. train-set과 test-set을 align한다.\n즉, train 데이터와 test 데이터에 둘다 공통적으로 있는 컬럼들의 값만 가져오려는것\n\"\"\"\napp_train, app_test=app_train.align(app_test,join='inner',axis=1)\n\n# TARGET변수 다시 추가\napp_train['TARGET']=train_labels\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T12:28:19.621891Z","iopub.status.idle":"2022-03-20T12:28:19.622424Z","shell.execute_reply.started":"2022-03-20T12:28:19.622173Z","shell.execute_reply":"2022-03-20T12:28:19.622204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train데이터와 Test데이터 컬럼 맞춰주기\ntrain 데이터와 test 데이터에는 동일한 feature가 있어야 한다.  \ntrain 데이터에 있는 카테고리변수의 유니크한 값 개수와 test 데이터에 있는 카테고리 변수의 유니크한 값 개수가 다른 변수들이 있어서 one-hot-encoding을 했더니, train에는 있는데 test에 없는 컬럼들이 생겨버림.  \n\n따라서 test 데이터에 없고 train에만 있는 컬럼을 삭제해야됨.  \n\n우선, train 데이터에서 TARGET 컬럼을 뽑아낸다. * TARGET 컬럼은 test데이터에 없어도 train 데이터에는 반드시 있어야하기 때문에  \n\nalign() 함수의 join메소드를 inner로 적용해서 교집합으로 있는 변수만 추린다.  ","metadata":{}},{"cell_type":"code","source":"# DAYS_BIRTH 컬럼에서는 이상치 없어보임\n(app_train['DAYS_BIRTH'] / -365).describe()","metadata":{"execution":{"iopub.status.busy":"2022-03-20T12:56:02.895082Z","iopub.execute_input":"2022-03-20T12:56:02.895361Z","iopub.status.idle":"2022-03-20T12:56:02.924175Z","shell.execute_reply.started":"2022-03-20T12:56:02.895329Z","shell.execute_reply":"2022-03-20T12:56:02.923371Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# DAYS_EMPLOYED는 이상치..\napp_train['DAYS_EMPLOYED'].describe()","metadata":{"execution":{"iopub.status.busy":"2022-03-20T12:56:13.665474Z","iopub.execute_input":"2022-03-20T12:56:13.665730Z","iopub.status.idle":"2022-03-20T12:56:13.685626Z","shell.execute_reply.started":"2022-03-20T12:56:13.665704Z","shell.execute_reply":"2022-03-20T12:56:13.684784Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"app_train[\"DAYS_EMPLOYED\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-03-20T12:47:53.803643Z","iopub.execute_input":"2022-03-20T12:47:53.803989Z","iopub.status.idle":"2022-03-20T12:47:53.827090Z","shell.execute_reply.started":"2022-03-20T12:47:53.803949Z","shell.execute_reply":"2022-03-20T12:47:53.826376Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Just out of curiousity, let's subset the anomalous clients and see if they tend to have higher or low rates of default than the rest of the clients.  \n- Only index 365243 has plus value. \n- anom : 365243\n- non_anom: != 365243","metadata":{}},{"cell_type":"code","source":"anom = app_train[app_train['DAYS_EMPLOYED'] == 365243]\nnon_anom = app_train[app_train['DAYS_EMPLOYED'] != 365243]\n\nprint('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))\nprint('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))\nprint('There are %d anomalous days of employment' % len(anom))","metadata":{"execution":{"iopub.status.busy":"2022-03-20T12:57:40.762000Z","iopub.execute_input":"2022-03-20T12:57:40.762286Z","iopub.status.idle":"2022-03-20T12:57:41.023552Z","shell.execute_reply.started":"2022-03-20T12:57:40.762259Z","shell.execute_reply":"2022-03-20T12:57:41.022712Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"이상치로 보이는 고객들이 대출을 상환하지못할 확률이 5.4%로 더 낮음.  \n이상치를 다루는 가장 안전한 방법은 결측치 채우듯이 채우는 방법  \n이 경우 모든 이상치들이 같은값을 갖고 있으므로, 다 같은 값으로 채울것이다.  \n이상값들이 중요해보이니, 머신러닝 모델에 이 이상값들을 임의로 채운것에대해 알려줄것이다.  ","metadata":{}},{"cell_type":"code","source":"app_test['DAYS_EMPLOYED_ANOM'] = app_test[\"DAYS_EMPLOYED\"] == 365243\napp_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\n\nprint('There are %d anomalies in the test data out of %d entries' % (app_test[\"DAYS_EMPLOYED_ANOM\"].sum(), len(app_test)))\n\n\n'''\nQ) \n\nanom_test = app_test['DAYS_EMPLOYED_ANOM'] = app_test[\"DAYS_EMPLOYED\"] == 365243\napp_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\n\nprint('There are %d anomalies in the test data out of %d entries' % (len(anom_test), len(app_test))) \n\n해도 되나?\n'''\nQ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 최소 20 최대 70, 11개의 간격으로 >> 총 10개로 그룹핑\n## 결과는 20이상 25미만, 25이상 30미만,,,, 으로 그룹핑됨. 단 (,)는 포함 [,]는 미포함을 의미. (20,25] 20이상 25미만 \nnp.linspace(20,70,num=11)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\ncut() 함수를 사용해서 5살 간격으로 나이대 그룹을 나눠보자. \n그다음, 각 나이대 별로 대출상환을 못하는 비율을 체크\n\"\"\"\n\nage_data=app_train[['TARGET','DAYS_BIRTH']]\nage_data['YEARS_BIRTH']=age_data['DAYS_BIRTH']/365\n\n# Bin the age data\n# \nage_data['YEARS_BINNED']=pd.cut(age_data['YEARS_BIRTH'],bins=np.linspace(20,70,num=11))\nage_data.head(10)","metadata":{},"execution_count":null,"outputs":[]}]}