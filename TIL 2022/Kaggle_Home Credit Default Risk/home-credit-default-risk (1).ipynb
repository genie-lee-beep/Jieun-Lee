{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np \nimport pandas as pd \n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n# File system manangement\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-20T21:20:16.595089Z","iopub.execute_input":"2022-03-20T21:20:16.595735Z","iopub.status.idle":"2022-03-20T21:20:16.615045Z","shell.execute_reply.started":"2022-03-20T21:20:16.595699Z","shell.execute_reply":"2022-03-20T21:20:16.614421Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"This time I transcripted by referring to the link below.  \nhttps://www.kaggle.com/code/willkoehrsen/start-here-a-gentle-introduction/notebook  \n\n\nSupervised: The labels are included in the training data and the goal is to train a model to learn to predict the labels from the features  \nClassification: The label is a binary variable, 0 (will repay loan on time), 1 (will have difficulty repaying loan)  \n\n\nFollow-up Notebooks\n\nFor those looking to keep working on this problem, I have a series of follow-up notebooks:\n\n- Manual Feature Engineering Part One  \n- Manual Feature Engineering Part Two  \n- Introduction to Automated Feature Engineering  \n- Advanced Automated Feature Engineering\n- Feature Selection\n- Intro to Model Tuning: Grid and Random Search\n- Automated Model Tuning\n- Model Tuning Results\n","metadata":{}},{"cell_type":"code","source":"# sklearn preprocessing for dealing with categorical variables\nfrom sklearn.preprocessing import LabelEncoder\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2022-03-20T21:20:16.616270Z","iopub.execute_input":"2022-03-20T21:20:16.617021Z","iopub.status.idle":"2022-03-20T21:20:17.654929Z","shell.execute_reply.started":"2022-03-20T21:20:16.616987Z","shell.execute_reply":"2022-03-20T21:20:17.654161Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Training data\napp_train = pd.read_csv('../input/home-credit/application_train.csv')\nprint('Training data shape: ', app_train.shape)\napp_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-20T21:20:17.656234Z","iopub.execute_input":"2022-03-20T21:20:17.656456Z","iopub.status.idle":"2022-03-20T21:20:23.439648Z","shell.execute_reply.started":"2022-03-20T21:20:17.656429Z","shell.execute_reply":"2022-03-20T21:20:23.439120Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Testing data features\napp_test = pd.read_csv('../input/homecredit/application_test.csv')\nprint('Testing data shape: ', app_test.shape)\napp_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-20T21:20:23.441150Z","iopub.execute_input":"2022-03-20T21:20:23.441527Z","iopub.status.idle":"2022-03-20T21:20:24.366085Z","shell.execute_reply.started":"2022-03-20T21:20:23.441491Z","shell.execute_reply":"2022-03-20T21:20:24.365373Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"app_train['TARGET'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-03-20T21:23:44.085825Z","iopub.execute_input":"2022-03-20T21:23:44.086136Z","iopub.status.idle":"2022-03-20T21:23:44.095634Z","shell.execute_reply.started":"2022-03-20T21:23:44.086093Z","shell.execute_reply":"2022-03-20T21:23:44.094794Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# print(app_train['TARGET'].astype(int).plot.hist());\napp_train['TARGET'].plot.hist();","metadata":{"execution":{"iopub.status.busy":"2022-03-20T21:20:24.383558Z","iopub.execute_input":"2022-03-20T21:20:24.383918Z","iopub.status.idle":"2022-03-20T21:20:24.649437Z","shell.execute_reply.started":"2022-03-20T21:20:24.383873Z","shell.execute_reply":"2022-03-20T21:20:24.648688Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"From this information, we see this is an imbalanced class problem. There are far more loans that were repaid on time than loans that were not repaid. ","metadata":{}},{"cell_type":"markdown","source":"### Examine Missing Values\nhttps://suhyun-cho.github.io/kaggle/kaggle-HomeCredit-default-risk-eda-and-FeatureEngineering_beginner/","metadata":{}},{"cell_type":"code","source":"# mis_val = df.isnill().sum()\n\n\n# We cannot check all misng values if there are lots of columns. \n# mis_val = app_train.isna().sum()\n","metadata":{"execution":{"iopub.status.busy":"2022-03-20T21:29:16.544332Z","iopub.execute_input":"2022-03-20T21:29:16.544764Z","iopub.status.idle":"2022-03-20T21:29:16.551979Z","shell.execute_reply.started":"2022-03-20T21:29:16.544723Z","shell.execute_reply":"2022-03-20T21:29:16.551261Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"When it comes time to build our machine learning models, we will have to fill in these missing values (known as imputation).   \nIn later work, we will use models such as XGBoost that can handle missing values with no need for imputation.   \nAnother option would be to drop columns with a high percentage of missing values, although it is impossible  \nto know ahead of time if these columns will be helpful to our model. \nTherefore, we will keep all of the columns for now.  ","metadata":{}},{"cell_type":"markdown","source":"### Column Types\n\nLet's look at the number of columns of each data type.  \nint64 and float64 are numeric variables (which can be either discrete or continuous).  \nobject columns contain strings and are categorical features.  ","metadata":{}},{"cell_type":"code","source":"app_train.dtypes.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-03-20T21:33:14.888015Z","iopub.execute_input":"2022-03-20T21:33:14.888519Z","iopub.status.idle":"2022-03-20T21:33:14.897632Z","shell.execute_reply.started":"2022-03-20T21:33:14.888476Z","shell.execute_reply":"2022-03-20T21:33:14.896879Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"Let's now look at the number of unique entities in each of the object(categorical)columns.","metadata":{}},{"cell_type":"code","source":"app_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T21:37:38.624796Z","iopub.execute_input":"2022-03-20T21:37:38.625317Z","iopub.status.idle":"2022-03-20T21:37:38.990183Z","shell.execute_reply.started":"2022-03-20T21:37:38.625279Z","shell.execute_reply":"2022-03-20T21:37:38.989463Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"### Encoding Categorical Variables","metadata":{}},{"cell_type":"code","source":"# 이 노트북에서는 범주형변수의 유니크한 값이 2개일경우 Label encoding을 사용하고\n#그 이상일 경우 One-hot encoding을 사용한다.\nle=LabelEncoder()\nle_count=0\n\n# 컬럼별로 iterate 돌기\nfor col in app_train:\n    # df[리스트] \n    # dataframe의 경우 \n    if app_train[col].dtype=='object':\n        # 데이터타입이 object이고 값의 종류가 두개 이하일경우,\n        if len(list(app_train[col].unique())) <=2:  \n            \n            # train과 test에 동일하게 라벨인코딩을 하기위해 train기준으로 fit한값을 train,test에 동일하게 transform해줌\n            le.fit(app_train[col])\n            \n            # train-set, test-set 둘다 Transform\n            app_train[col]=le.transform(app_train[col])\n            app_test[col]=le.transform(app_test[col])\n            \n            # 라벨인코딩을 한 컬럼이 몇개인지 카운트\n            le_count+=1                                                    # Q 어떻게 카운트하는 건지?\nprint('%d columns were label encoded.' % le_count)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T21:23:57.378092Z","iopub.execute_input":"2022-03-20T21:23:57.378769Z","iopub.status.idle":"2022-03-20T21:23:57.758803Z","shell.execute_reply.started":"2022-03-20T21:23:57.378718Z","shell.execute_reply":"2022-03-20T21:23:57.757973Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# 위에서 Label-encoding적용 안한 나머지 범주형 변수에 One-hot encoding 적용\n# 위에서 이미 OBJECT 값이 2개인경우는 Label encoding 했으니까, 여기서는 바로 나머지 obect애 대해서는 get.cummies를 쫙 해준다.  \napp_train=pd.get_dummies(app_train)\napp_test=pd.get_dummies(app_test)\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-20T21:20:25.390977Z","iopub.execute_input":"2022-03-20T21:20:25.391226Z","iopub.status.idle":"2022-03-20T21:20:25.397063Z","shell.execute_reply.started":"2022-03-20T21:20:25.391199Z","shell.execute_reply":"2022-03-20T21:20:25.395688Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# TARGET변수는 train데이터에만 있지만 필요한 변수이기때문에 따로 빼두고나서 다시추가할것\ntrain_labels=app_train['TARGET']\n\n\"\"\"\n컬럼 수가 비교적 작은 train_set을 기준으로 컬럼수를 align해서 맞춰 준다. train-set과 test-set을 align한다.\n즉, train 데이터와 test 데이터에 둘다 공통적으로 있는 컬럼들의 값만 가져오려는것\n\"\"\"\napp_train, app_test=app_train.align(app_test,join='inner',axis=1)\n\n# TARGET변수 다시 추가\napp_train['TARGET']=train_labels\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T21:20:25.398220Z","iopub.status.idle":"2022-03-20T21:20:25.398649Z","shell.execute_reply.started":"2022-03-20T21:20:25.398424Z","shell.execute_reply":"2022-03-20T21:20:25.398446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train데이터와 Test데이터 컬럼 맞춰주기\ntrain 데이터와 test 데이터에는 동일한 feature가 있어야 한다.  \ntrain 데이터에 있는 카테고리변수의 유니크한 값 개수와 test 데이터에 있는 카테고리 변수의 유니크한 값 개수가 다른 변수들이 있어서 one-hot-encoding을 했더니, train에는 있는데 test에 없는 컬럼들이 생겨버림.  \n\n따라서 test 데이터에 없고 train에만 있는 컬럼을 삭제해야됨.  \n\n우선, train 데이터에서 TARGET 컬럼을 뽑아낸다. * TARGET 컬럼은 test데이터에 없어도 train 데이터에는 반드시 있어야하기 때문에  \n\nalign() 함수의 join메소드를 inner로 적용해서 교집합으로 있는 변수만 추린다.  ","metadata":{}},{"cell_type":"code","source":"# DAYS_BIRTH 컬럼에서는 이상치 없어보임\n(app_train['DAYS_BIRTH'] / -365).describe()","metadata":{"execution":{"iopub.status.busy":"2022-03-20T21:20:25.399910Z","iopub.status.idle":"2022-03-20T21:20:25.400297Z","shell.execute_reply.started":"2022-03-20T21:20:25.400091Z","shell.execute_reply":"2022-03-20T21:20:25.400112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DAYS_EMPLOYED는 이상치..\napp_train['DAYS_EMPLOYED'].describe()","metadata":{"execution":{"iopub.status.busy":"2022-03-20T21:20:25.401415Z","iopub.status.idle":"2022-03-20T21:20:25.401820Z","shell.execute_reply.started":"2022-03-20T21:20:25.401605Z","shell.execute_reply":"2022-03-20T21:20:25.401627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"app_train[\"DAYS_EMPLOYED\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-03-20T21:20:25.403271Z","iopub.status.idle":"2022-03-20T21:20:25.404203Z","shell.execute_reply.started":"2022-03-20T21:20:25.403935Z","shell.execute_reply":"2022-03-20T21:20:25.403963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Just out of curiousity, let's subset the anomalous clients and see if they tend to have higher or low rates of default than the rest of the clients.  \n- Only index 365243 has plus value. \n- anom : 365243\n- non_anom: != 365243","metadata":{}},{"cell_type":"code","source":"anom = app_train[app_train['DAYS_EMPLOYED'] == 365243]\nnon_anom = app_train[app_train['DAYS_EMPLOYED'] != 365243]\n\nprint('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))\nprint('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))\nprint('There are %d anomalous days of employment' % len(anom))","metadata":{"execution":{"iopub.status.busy":"2022-03-20T21:20:25.405644Z","iopub.status.idle":"2022-03-20T21:20:25.406195Z","shell.execute_reply.started":"2022-03-20T21:20:25.405927Z","shell.execute_reply":"2022-03-20T21:20:25.405952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"이상치로 보이는 고객들이 대출을 상환하지못할 확률이 5.4%로 더 낮음.  \n이상치를 다루는 가장 안전한 방법은 결측치 채우듯이 채우는 방법  \n이 경우 모든 이상치들이 같은값을 갖고 있으므로, 다 같은 값으로 채울것이다.  \n이상값들이 중요해보이니, 머신러닝 모델에 이 이상값들을 임의로 채운것에대해 알려줄것이다.  ","metadata":{}},{"cell_type":"code","source":"app_test['DAYS_EMPLOYED_ANOM'] = app_test[\"DAYS_EMPLOYED\"] == 365243\napp_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\n\nprint('There are %d anomalies in the test data out of %d entries' % (app_test[\"DAYS_EMPLOYED_ANOM\"].sum(), len(app_test)))\n\n\n'''\nQ) \n\nanom_test = app_test['DAYS_EMPLOYED_ANOM'] = app_test[\"DAYS_EMPLOYED\"] == 365243\napp_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\n\nprint('There are %d anomalies in the test data out of %d entries' % (len(anom_test), len(app_test))) \n\n해도 되나?\n'''\nQ","metadata":{"execution":{"iopub.status.busy":"2022-03-20T21:20:25.407459Z","iopub.status.idle":"2022-03-20T21:20:25.407841Z","shell.execute_reply.started":"2022-03-20T21:20:25.407637Z","shell.execute_reply":"2022-03-20T21:20:25.407658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 최소 20 최대 70, 11개의 간격으로 >> 총 10개로 그룹핑\n## 결과는 20이상 25미만, 25이상 30미만,,,, 으로 그룹핑됨. 단 (,)는 포함 [,]는 미포함을 의미. (20,25] 20이상 25미만 \nnp.linspace(20,70,num=11)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T21:20:25.408973Z","iopub.status.idle":"2022-03-20T21:20:25.409376Z","shell.execute_reply.started":"2022-03-20T21:20:25.409145Z","shell.execute_reply":"2022-03-20T21:20:25.409165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\ncut() 함수를 사용해서 5살 간격으로 나이대 그룹을 나눠보자. \n그다음, 각 나이대 별로 대출상환을 못하는 비율을 체크\n\"\"\"\n\nage_data=app_train[['TARGET','DAYS_BIRTH']]\nage_data['YEARS_BIRTH']=age_data['DAYS_BIRTH']/365\n\n# Bin the age data\n# \nage_data['YEARS_BINNED']=pd.cut(age_data['YEARS_BIRTH'],bins=np.linspace(20,70,num=11))\nage_data.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T21:20:25.410671Z","iopub.status.idle":"2022-03-20T21:20:25.411074Z","shell.execute_reply.started":"2022-03-20T21:20:25.410864Z","shell.execute_reply":"2022-03-20T21:20:25.410886Z"},"trusted":true},"execution_count":null,"outputs":[]}]}