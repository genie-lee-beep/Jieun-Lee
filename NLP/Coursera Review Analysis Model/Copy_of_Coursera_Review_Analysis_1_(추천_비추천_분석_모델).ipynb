{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Coursera Review Analysis 1 (추천/비추천 분석 모델)",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The sentiment of reviews is binary, meaning the review rating <=2 results in a sentiment score of 0, and rating >=4 have a sentiment score of 1. "
      ],
      "metadata": {
        "id": "0Q6RiEAxhw0l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "dXjT3aPJG3Nv",
        "outputId": "ad181955-76cd-4ea6-f47e-ad73e4b8dd9a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-194430c18ba4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'reviews.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'reviews.csv'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "dataset = pd.read_csv('reviews.csv')\n",
        "dataset.drop(columns=\"Id\", inplace=True)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.drop_duplicates(ignore_index=True)\n",
        "dataset"
      ],
      "metadata": {
        "id": "Q5fZ_lipHE7C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "ce59f78b-d59e-44ba-c7e0-0cf0b5cb0349"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-0edf929c03ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['Label'].hist()"
      ],
      "metadata": {
        "id": "cT2rrAOwHt_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['Label'].value_counts()"
      ],
      "metadata": {
        "id": "W8jWCwmeHvBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 어중간한 3점 삭제해 버리기\n",
        "rating_3 = dataset[dataset['Label']==3].index\n",
        "del_3 = dataset.drop(rating_3)\n",
        "dataset=del_3"
      ],
      "metadata": {
        "id": "HD7MFybnHwKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['Label'].value_counts()"
      ],
      "metadata": {
        "id": "sSE_RnnTHw8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 현재 4~5점(긍정)에 데이터가 너무 많음\n",
        "#1~2점(부정)과 4~5점(긍정) 데이터 비율을 맞춰주기\n",
        "def reduce_size(x):\n",
        "    if len(x.index) > 70000:\n",
        "        return x.sample(n=int(len(x.index)/35)) #70000/2200\n",
        "    elif len(x.index) > 17000:\n",
        "        return x.sample(n=int(len(x.index)/7)) #17000/2200\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "dataset = dataset.groupby('Label').apply(reduce_size).reset_index(drop=True)\n",
        "dataset['Label'].value_counts()\n",
        "\n",
        "# 4~5점 = 4601 / 1~2점 = 4622"
      ],
      "metadata": {
        "id": "hL0cmYWNHyQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.shape"
      ],
      "metadata": {
        "id": "Hfh7tNeKHzaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "PE8cKenanUj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ready to split 'train set' and  'test set'\n",
        "X = dataset['Review']\n",
        "y = dataset['Label']"
      ],
      "metadata": {
        "id": "iXO9y2ZPH0ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,  y,  test_size=0.5,shuffle=True,  random_state=1004)\n",
        "             \n",
        "print('X_train shape:', X_train.shape)\n",
        "\n",
        "print('X_test shape:', X_test.shape)\n",
        "\n",
        "print('y_train shape:', y_train.shape)\n",
        "\n",
        "print('y_test shape:', y_test.shape)"
      ],
      "metadata": {
        "id": "mdGstNf3H1ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(y_train))\n",
        "print(type(X_test))"
      ],
      "metadata": {
        "id": "KlFPQlynzIR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train과 test의 데이터에 각 점수들이 반반씩 잘 분포 되었음.\n",
        "print('train set:', y_train.value_counts())\n",
        "print('----------------------------')\n",
        "print('test set:', y_test.value_counts())"
      ],
      "metadata": {
        "id": "7akPgRdWH2SI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "AK7JkJ7fTo-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.dtypes"
      ],
      "metadata": {
        "id": "J8UeE9_qH3UW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#replace has a default arg inplace=False. \n",
        "#Thus, your results will not affect each other and you will have to combine them into dataset['label']=dataset['label'].replace([1,2,4,5],[0,0,1,1]) or \n",
        "#dataset['label'].replace([1,2,4,5],[0,0,1,1],inplace=True)\n",
        "\n",
        "dataset['Label']=dataset['Label'].replace([1,2,4,5],[0,0,1,1])"
      ],
      "metadata": {
        "id": "-pOMoABJH4d0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['Label'].value_counts()"
      ],
      "metadata": {
        "id": "QGk0XdPjH5pB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "dwcgmY0jH7AY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test"
      ],
      "metadata": {
        "id": "QxUQENVGH8QU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# concat()dms df와 series에 다 적용가능\n",
        "train_set = pd.concat([X_train, y_train], axis = 1)\n",
        "print(train_set)"
      ],
      "metadata": {
        "id": "c6yqd83KH-4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set.head(8)"
      ],
      "metadata": {
        "id": "-edkDx-US3Kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set['Review'][7]"
      ],
      "metadata": {
        "id": "1L6p_28fRtwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "example1 = BeautifulSoup(train_set['Review'][7], \"html5lib\")\n",
        "print(train_set['Review'][:20])\n",
        "\n",
        "\n",
        "example1.get_text()[:10]"
      ],
      "metadata": {
        "id": "DYPyTfyMH_7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "letters_only= re.sub('[^a-zA-Z]', ' ', example1.get_text())\n",
        "letters_only[:10]"
      ],
      "metadata": {
        "id": "FPbU4hdlIA0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# making lower case\n",
        "lower_case = letters_only.lower()\n",
        "# lower_case = letters_only['Review'].str.lower()      # Q str?"
      ],
      "metadata": {
        "id": "vbpNKdUeIBq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing\n",
        "words = lower_case.split()\n",
        "print(len(words))\n",
        "words[:10]"
      ],
      "metadata": {
        "id": "rHmuYyKIIDS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DOwnload for preprocessing the text\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "VCjxGGNEpDtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "stopwords.words('english')[:20]"
      ],
      "metadata": {
        "id": "FwE9zEW3pFoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stopwords"
      ],
      "metadata": {
        "id": "DJSWbZp8xmRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokens exceot for stopwords \n",
        "words = [w for w in words if not w in stopwords.words('english')]\n",
        "print(len(words))\n",
        "words[:10]"
      ],
      "metadata": {
        "id": "sIL5hU2spINg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SnowballStemmer\n",
        "\n",
        "print(\"The stemmed form of studying is: {}\".format(stemmer.stem(\"studying\")))  \n",
        "print(\"The stemmed form of studies is: {}\".format(stemmer.stem(\"studies\")))  \n",
        "print(\"The stemmed form of study is: {}\".format(stemmer.stem(\"study\")))  \n",
        "\n",
        ">The stemmed form of studying is: studi  \n",
        ">The stemmed form of studies is: studi  \n",
        ">The stemmed form of study is: studi  "
      ],
      "metadata": {
        "id": "-Io7tJj9P8O6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Before stemmer\n",
        "words[:30]"
      ],
      "metadata": {
        "id": "EL82rIoiNv9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "wordss = [stemmer.stem(w) for w in words]\n",
        "# After stemmer\n",
        "words[:10]"
      ],
      "metadata": {
        "id": "es_vbWqfpJ5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = [wordnet_lemmatizer.lemmatize(w) for w in words]\n",
        "# 처리 후 단어\n",
        "words[:10]"
      ],
      "metadata": {
        "id": "_fa4bhylpLgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def review_to_words(raw_review ):\n",
        "    # 1. HTML 제거\n",
        "    review_text = BeautifulSoup(raw_review, 'html.parser').get_text()\n",
        "    # 2. 영문자가 아닌 문자는 공백으로 변환\n",
        "    letters_only = re.sub('[^a-zA-Z]', ' ', review_text)\n",
        "    # 3. 소문자 변환\n",
        "    words = letters_only.lower().split()\n",
        "    # 4. 파이썬에서는 리스트보다 세트로 찾는게 훨씬 빠르다.\n",
        "    # stopwords 를 세트로 변환한다.\n",
        "    stops = set(stopwords.words('english'))\n",
        "    # 5. Stopwords 불용어 제거\n",
        "    meaningful_words = [w for w in words if not w in stops]\n",
        "    # 6. 어간추출\n",
        "    stemming_words = [stemmer.stem(w) for w in meaningful_words]\n",
        "    # 7. 공백으로 구분된 문자열로 결합하여 결과를 반환\n",
        "    return( ' '.join(stemming_words) )"
      ],
      "metadata": {
        "id": "4eNrG-FTpNSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from multiprocessing import Pool\n",
        "import numpy as np\n",
        "\n",
        "def _apply_df(args):\n",
        "    df, func, kwargs = args\n",
        "    return df.apply(func, **kwargs)\n",
        "\n",
        "def apply_by_multiprocessing(df, func, **kwargs):\n",
        "    # 키워드 항목 중 workers 파라메터를 꺼냄\n",
        "    workers = kwargs.pop('workers')\n",
        "    # 위에서 가져온 workers 수로 프로세스 풀을 정의\n",
        "    pool = Pool(processes=workers)\n",
        "    # 실행할 함수와 데이터프레임을 워커의 수 만큼 나눠 작업\n",
        "    result = pool.map(_apply_df, [(d, func, kwargs)\n",
        "            for d in np.array_split(df, workers)])\n",
        "    pool.close()\n",
        "    # 작업 결과를 합쳐서 반환\n",
        "    return pd.concat(list(result))"
      ],
      "metadata": {
        "id": "PiHiDzLipPIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%time clean_train_reviews = apply_by_multiprocessing(\\\n",
        "    train_set['Review'], review_to_words, workers=4)  "
      ],
      "metadata": {
        "id": "C1IwL7YJpRS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_train_reviews"
      ],
      "metadata": {
        "id": "VYFaYxQtrzAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%time clean_test_reviews = apply_by_multiprocessing(\\\n",
        "   X_test, review_to_words, workers=4)  "
      ],
      "metadata": {
        "id": "MIXtUgqaQ3h_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag-of-words model - Wikipedia  \n",
        "다음의 두 문장이 있다고 하자,  \n",
        "\n",
        "(1) John likes to watch movies. Mary likes movies too.  \n",
        "(2) John also likes to watch football games.  \n",
        "위 두 문장을 토큰화 하여 가방에 담아주면 다음과 같다.  \n",
        "\n",
        "[\n",
        "    \"John\",  \n",
        "    \"likes\",  \n",
        "    \"to\",  \n",
        "    \"watch\",  \n",
        "    \"movies\",  \n",
        "    \"Mary\",    \n",
        "    \"too\",  \n",
        "    \"also\",  \n",
        "    \"football\",  \n",
        "    \"games\"  \n",
        "]  \n",
        "그리고 배열의 순서대로 가방에서 각 토큰이 몇 번 등장하는지 횟수를 세어준다.  \n",
        "  \n",
        "(1) [1, 2, 1, 1, 2, 1, 1, 0, 0, 0]    \n",
        "(2) [1, 1, 1, 1, 0, 0, 0, 1, 1, 1]    \n",
        "=> 머신러닝 알고리즘이 이해할 수 있는 형태로 바꿔주는 작업이다.  \n",
        "  \n",
        "단어 가방을 n-gram을 사용해 bigram 으로 담아주면 다음과 같다.  \n",
        "\n",
        "[  \n",
        "    \"John likes\",  \n",
        "    \"likes to\",  \n",
        "    \"to watch\",  \n",
        "    \"watch movies\",  \n",
        "    \"Mary likes\",  \n",
        "    \"likes movies\",  \n",
        "    \"movies too\",  \n",
        "]  \n",
        "=> 여기에서는 CountVectorizer를 통해 위 작업을 한다.  \n",
        "\n",
        "### <CountVectorizer 파라미터>\n",
        "출처: https://chan-lab.tistory.com/27 [은공지능 공작소]\n",
        "1.  analyzer 파라미터는 학습단위를 결정하는 파라미터입니다.\n",
        "- word, char 2가지 옵션 정도를 고려해볼 수 있습니다.\n",
        "- analyzer = 'word'라고 설정시, 학습의 단위를 단어로 설정합니다. (ex - home, go, my ...)\n",
        "- analyzer = 'char'라고 설정시, 학습의 단위를 글자로 설정합니다.(ex - a, b, c, d ...)\n",
        "- char 기반의 analyzer을 하면, ('a', 1), ('b', 2), ('c', 3), ('e', 4), ('g', 5), ('h', 6), ('i', 7)... 형태로 출력되기 때문에 실제 모델에서\n",
        "좋은 성능을 보일지 의문이 남을 수도 있을 것 같습니다.  \n",
        "결론부터 말씀드리면, 상당히 좋은 결과를 내놓을 수 있습니다.  \n",
        "제가 CNN 모델에서 char analyzer로 학습을 시킨 적이 있었는데,\n",
        "생각보다 괜찮은 결과가 나왔던 것으로 기억합니다.  \n",
        "그러니 모델의 성능을 올리기 위해서, word 기반 방법만 고집할 것이 아니라 char 기반의 방법도 시도해 볼 만하다고 생각합니다.    \n",
        "\n",
        "2.min_df 파라미터 \n",
        "- 최소 빈도값을 설정해주는 파라미터\n",
        "최소 DF가 2로 설정되었으니, 1인 것들은 모두 탈락하게 됩니다.\n",
        "\n",
        "3. ngram_range 파라미터   \n",
        "- 만약 ngram_range = (1, 3)라고 한다면, 단어의 묶음을 1개부터 3개까지 설정하라는 뜻입니다.\n",
        "- 단어사전에는 1개 단어묶음도 있고, 2개 단어묶음도, 3개 단어묶음도 존재하게 되겠죠.\n"
      ],
      "metadata": {
        "id": "ICr7PPIbu34f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "vectorizer = CountVectorizer(analyzer = 'word', \n",
        "                             tokenizer = None,\n",
        "                             preprocessor = None, \n",
        "                             stop_words = None, \n",
        "                             min_df = 2, # 토큰이 나타날 최소 문서 개수\n",
        "                             ngram_range=(1, 3),\n",
        "                             max_features = 20000\n",
        "                            )\n",
        "vectorizer"
      ],
      "metadata": {
        "id": "z_vY2m36pTZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pipeline\n",
        "\n",
        "https://deepinsight.tistory.com/173\n"
      ],
      "metadata": {
        "id": "A-MmUCRo5o2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 여기에서는 하나의 과정만 묶어주어 pipeline이 불필요 할 수도 있습니다.\n",
        "# pipeline은 feature engineering의 여러 과정을 묶어 줄 때 사용합니다.\n",
        "pipeline = Pipeline([\n",
        "    ('vect', vectorizer),\n",
        "])  "
      ],
      "metadata": {
        "id": "a7WygmMopVmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "CountVectorizer는 여러 메소드 목록을 제공하고 있습니다.\n",
        "메소드 중에서 fit_transform 을 하면  term-document matrix 를 반환하는 것을 볼 수 있습니다.\n",
        "파이프라인으로 fit_transform을 해주게 되는데 해당 과정의 결과로 피처의 목록과 등장여부를 알 수 있습니다.\n",
        "'''\n",
        "%time train_data_features = pipeline.fit_transform(clean_train_reviews)\n",
        "train_data_features"
      ],
      "metadata": {
        "id": "ehK3-lSdpXVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorize를 한 결과 \n",
        "train_data_features.shape"
      ],
      "metadata": {
        "id": "0dJkhVSqpZfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "get_feature_names() 라는 것도 CountVectorizer 에서 사용할 수 있는 메소드 중 하나입니다.\n",
        "위에서 정의한 CountVectorizer 에 데이터를 넣어주고 fit 혹은 fit_transform 을 하게 되면 벡터화와 함께 사전(get_feature_names)을 생성하게 됩니다.\n",
        "'''\n",
        "vocab = vectorizer.get_feature_names()\n",
        "print(len(vocab))\n",
        "vocab[:10]"
      ],
      "metadata": {
        "id": "wevNpsJUpab-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 벡터화 된 피처를 확인해 봄\n",
        "import numpy as np\n",
        "dist = np.sum(train_data_features, axis=0)\n",
        "    \n",
        "for tag, count in zip(vocab, dist):\n",
        "    print(count, tag)\n",
        "    \n",
        "pd.DataFrame(dist, columns=vocab)"
      ],
      "metadata": {
        "id": "uuObyZlapcMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(train_data_features[:10].toarray(), columns=vocab).head()"
      ],
      "metadata": {
        "id": "XWFxutl8pdsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 예측"
      ],
      "metadata": {
        "id": "qbCW6qwnDTnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 체크해 보고 싶은 Review(string)를 Series에 담아준다.\n",
        "# reveiw_sample1 = \"I don't like to spend money to bad lecture, but Cousera MOOC is really helpful to upgrade my understanding of U.S history.\"\n",
        "\n",
        "reveiw_sample1 = \"I I don't understand \"\n",
        "sample1 = pd.Series(reveiw_sample1)"
      ],
      "metadata": {
        "id": "nD1zrwR0ph5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 데이터를 벡터화 함\n",
        "%time test_data_feature = pipeline.transform(sample1)\n",
        "\n",
        "test_data_feature = test_data_feature.toarray()"
      ],
      "metadata": {
        "id": "rlCjl13Xpo90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_feature"
      ],
      "metadata": {
        "id": "lDWvPXbRqV39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clean_train_reviews에 대한 vocab \n",
        "# 벡터화 하며 만든 사전에서 해당 단어가 무엇인지 찾아볼 수 있다.\n",
        "# vocab = vectorizer.get_feature_names()\n",
        "# vocab[8], vocab[2558], vocab[2559], vocab[2560]"
      ],
      "metadata": {
        "id": "v724KHZqqY-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 데이터를 넣고 예측한다.\n",
        "result = model.predict(test_data_feature)\n",
        "# result[:100] # 100개의 리뷰에 대해서 뽑을 때 \n",
        "# 여기서는 test_data_feature 하나니까 결과값이 1개 나오는 것임 \n",
        "print(\"결과:\",result)  #긍정1 부정0"
      ],
      "metadata": {
        "id": "kWSiJLLDqdj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델의 정확도를 높이기 위한 개선 방법\n",
        "1. Stopwords 라이브러리 추가 \n",
        "2. Convector 파라미터 바꿔가면서 해보기 "
      ],
      "metadata": {
        "id": "wBKCgf6x2af3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PcdMVLQLpxyn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}